{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 357
    },
    "colab_type": "code",
    "id": "_NGgMALkxVgy",
    "outputId": "8fd43853-9f5e-4e80-b4f2-3c1a6be19362"
   },
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "i4CEnhT0xk4n",
    "outputId": "8c6b4e22-515f-41c8-ebdf-e19b81813e98"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/isabellanguyen/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:5: DeprecationWarning: use options instead of chrome_options\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "def chromedriver_set_options(chromedriver_path):\n",
    "    chrome_options = webdriver.ChromeOptions()\n",
    "    chrome_options.add_argument('--headless')\n",
    "    chrome_options.add_argument('--no-sandbox')\n",
    "    chrome_options.add_argument('--disable-dev-shm-usage')\n",
    "    wd = webdriver.Chrome(chromedriver_path, chrome_options=chrome_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "F4Qkqloz0yjS",
    "outputId": "dda61b8a-30cc-45d6-9114-7429e6588dfb"
   },
   "outputs": [],
   "source": [
    "def log_into_linkedin(username, password):\n",
    "    wd.get(\"https://www.linkedin.com/login\")\n",
    "    username_field = wd.find_element_by_id('username')\n",
    "    username_field.clear()\n",
    "    username_field.send_keys(username)\n",
    "    password_field = wd.find_element_by_id('password')\n",
    "    password_field.clear()\n",
    "    password_field.send_keys(password)\n",
    "    submit_button = wd.find_element_by_css_selector('button')\n",
    "    submit_button.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uLrWhRKv4Yo9"
   },
   "outputs": [],
   "source": [
    "def search_for_searchfield():\n",
    "    wd.implicitly_wait(10)\n",
    "    try:\n",
    "      search_field = wd.find_element_by_css_selector('.search-global-typeahead__input')\n",
    "    except:\n",
    "      if wd.title == 'Security Verification | LinkedIn':\n",
    "        # LinkedIn is asking for a verification code. Check your email and input the code here\n",
    "        code = input('Enter the verification code: ')\n",
    "        verification_input = wd.find_element_by_css_selector('.input_verification_pin')\n",
    "        verification_input.clear()\n",
    "        verification_input.send_keys(code)\n",
    "        submit_button = wd.find_element_by_id('email-pin-submit-button')\n",
    "        submit_button.click()\n",
    "        # Implicit wait already set to 10 seconds\n",
    "        search_field = wd.find_element_by_css_selector('.search-global-typeahead__input')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PUOMR6VlC-_L"
   },
   "outputs": [],
   "source": [
    "\n",
    "def partially_extract_jobs_on_page_1(search_url):\n",
    "    \n",
    "    index = 0\n",
    "    result_list = []\n",
    "    global jobs\n",
    "    \n",
    "    wd.get(search_url)\n",
    "\n",
    "    while True:\n",
    "        \n",
    "        print('Scraping job #' + str(index))\n",
    "        job = {}\n",
    "        time.sleep(1)    \n",
    "        \n",
    "        try:\n",
    "            result_list = wd.find_elements_by_css_selector('.job-card-list--underline-title-on-hover')\n",
    "            item = result_list[index]\n",
    "            \n",
    "            company_element = item.find_element_by_css_selector('.job-card-container__company-name')\n",
    "            company = company_element.text\n",
    "            \n",
    "            title_element = item.find_element_by_css_selector('.job-card-list__title')\n",
    "            title = title_element.text\n",
    "            \n",
    "            location_element = item.find_element_by_css_selector('.artdeco-entity-lockup__caption')\n",
    "            location = location_element.text\n",
    "            \n",
    "            url_element = item.find_element_by_css_selector('.job-card-container__link')\n",
    "            url = url_element.get_attribute('href')\n",
    "            \n",
    "            job = {\"company\": company, \"title\": title, \"location\": location, \"href\": url}\n",
    "            jobs.append(job) \n",
    "        \n",
    "        except:\n",
    "            \n",
    "            print('Error on job # ' + str(index))\n",
    "        \n",
    "        if index >= len(result_list) - 1:\n",
    "            break\n",
    "        \n",
    "        index = index + 1\n",
    "        wd.execute_script(\"return arguments[0].scrollIntoView();\", result_list[index]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "v437-osLI7q3"
   },
   "outputs": [],
   "source": [
    "def partially_extract_jobs_on_subsequent_pages(num_of_pages, search_url):\n",
    "    \n",
    "    for i in range(1, num_of_pages):\n",
    "        \n",
    "        extract_jobs_on_page(search_url + '&start=' + str(i * 25))\n",
    "        \n",
    "        time.sleep(3)\n",
    "        print('Successfully scraped page ' + str(i+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "ILQBAwYlDOa5",
    "outputId": "56b8cc13-891c-426b-c7f4-d970eff4f26f"
   },
   "outputs": [],
   "source": [
    "def fully_extract_jobs_on_all_pages():\n",
    "    \n",
    "    for job in jobs:\n",
    "        wd.get(job['href'])\n",
    "  \n",
    "  ## extracting salary estimate\n",
    "        \n",
    "        try:\n",
    "            salary = wd.find_element_by_css_selector('.salary-main-rail__data-amount').text\n",
    "            job['salary'] = salary\n",
    "\n",
    "  ## setting salary to some default value if salary information is unavailable \n",
    "\n",
    "        except:\n",
    "            job['salary'] = -1\n",
    "  \n",
    "        try:\n",
    "            description = wd.find_element_by_css_selector('.jobs-description-content__text').text\n",
    "            job['description'] = description\n",
    "\n",
    "        except:\n",
    "            job['description'] = -1\n",
    "\n",
    "        try:\n",
    "            skills_list = []\n",
    "            for skill in wd.find_elements_by_xpath(\".//div[@id='job-details']//li\"):\n",
    "                skills_list.append(skill.get_attribute('innerHTML'))\n",
    "            job['skills'] = skills_list\n",
    "\n",
    "        except:\n",
    "            job['skills'] = -1\n",
    "\n",
    "        try:\n",
    "            details = wd.find_element_by_class_name('jobs-description-details').text\n",
    "            job['details'] = details\n",
    "\n",
    "        except:\n",
    "            job['details'] = -1\n",
    "\n",
    "        print('successfully scraped job ' + str(jobs.index(job)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scraper(chromedriver_path, username, password, search_url, num_of_pages):\n",
    "    \n",
    "    chromedriver_set_options(chromedriver_path)\n",
    "    \n",
    "    log_into_linkedin(username, password)\n",
    "    \n",
    "    search_for_searchfield()\n",
    "    \n",
    "    partially_extract_jobs_on_page_1(search_url)\n",
    "    \n",
    "    partially_extract_jobs_on_subsequent_pages(num_of_pages, search_url)\n",
    "    \n",
    "    fully_extract_jobs_on_all_pages()\n",
    "    \n",
    "    df = pd.DataFrame(jobs)\n",
    "    \n",
    "    return df\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Complicated LinkedIn Job Scrape with Selenium and Python - Capturing \"Infinite Scrolling\" content.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
